{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Thermodynamic Scaling Analysis of Language Models\n",
       "\n",
       "This notebook performs comprehensive analysis of thermodynamic properties in transformer-based language models, focusing on detecting and characterizing quantum-like criticality and scaling laws.\n",
       "\n",
       "Authors: D. Samuel Hodge"
      ]
       },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Setup and Imports"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Select a model and layer for RG analysis\n",
       "model_name = \"gpt2\"\n",
       "layer_name = \"layer_0\"\n",
       "matrix_type = \"self_attention.query\"\n",
       "\n",
       "# Extract the weight matrix\n",
       "weight_matrix = model_weights[model_name][\"weights\"][layer_name][matrix_type]\n",
       "\n",
       "# Perform RG flow analysis\n",
       "rg_results = ts.theoretical_extensions.renormalization_group.renormalization_flow(\n",
       "    weight_matrix, temps, n_steps=5, block_size=2\n",
       ")\n",
       "\n",
       "# Analyze correlation length\n",
       "correlation_lengths = []\n",
       "for temp in temps:\n",
       "    corr_length = ts.theoretical_extensions.renormalization_group.calculate_correlation_length(\n",
       "        weight_matrix, temp\n",
       "    )\n",
       "    correlation_lengths.append(corr_length)\n",
       "\n",
       "rg_results[\"correlation_lengths\"] = correlation_lengths\n",
       "\n",
       "# Save results\n",
       "with open(os.path.join(RESULTS_DIR, \"rg_results.pkl\"), \"wb\") as f:\n",
       "    pickle.dump(rg_results, f)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 7. Replica Theory Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Select a model and layer for replica theory analysis\n",
       "model_name = \"gpt2\"\n",
       "layer_name = \"layer_0\"\n",
       "matrix_type = \"self_attention.query\"\n",
       "\n",
       "# Extract the weight matrix\n",
       "weight_matrix = model_weights[model_name][\"weights\"][layer_name][matrix_type]\n",
       "\n",
       "# Analyze replica symmetry breaking across temperature range\n",
       "rsb_results = []\n",
       "for temp in temps:\n",
       "    rsb = ts.theoretical_extensions.replica_theory.replica_symmetry_breaking(\n",
       "        weight_matrix, temp, n_replicas=5, n_samples=500\n",
       "    )\n",
       "    rsb_results.append(rsb)\n",
       "\n",
       "# Analyze free energy landscape for a specific temperature\n",
       "free_energy_landscape = ts.theoretical_extensions.replica_theory.free_energy_landscape(\n",
       "    weight_matrix, temperature=0.5, n_samples=1000\n",
       ")\n",
       "\n",
       "# Save results\n",
       "replica_results = {\n",
       "    \"temperatures\": temps,\n",
       "    \"rsb_results\": rsb_results,\n",
       "    \"free_energy_landscape\": free_energy_landscape\n",
       "}\n",
       "\n",
       "with open(os.path.join(RESULTS_DIR, \"replica_results.pkl\"), \"wb\") as f:\n",
       "    pickle.dump(replica_results, f)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 8. Methodological Enhancements"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 8.1 Eigenvalue Spectrum Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Collect eigenvalue data for different models and layers\n",
       "eigenvalue_data = {}\n",
       "\n",
       "for model_name in MODELS[:3]:  # Limit to first few models for brevity\n",
       "    eigenvalue_data[model_name] = {}\n",
       "    \n",
       "    for layer_name, layer_weights in model_weights[model_name][\"weights\"].items():\n",
       "        if not isinstance(layer_weights, dict):\n",
       "            continue\n",
       "            \n",
       "        for matrix_type in LAYER_TYPES[:2]:  # Limit to query and key matrices\n",
       "            if matrix_type not in layer_weights:\n",
       "                continue\n",
       "                \n",
       "            weight_matrix = layer_weights[matrix_type]\n",
       "            eigenvalues, hist, bin_edges = ts.methodological_enhancements.eigenvalue_analysis.eigenvalue_distribution(\n",
       "                weight_matrix, bins=50\n",
       "            )\n",
       "            \n",
       "            # Calculate spectral density\n",
       "            energy_points, density = ts.methodological_enhancements.eigenvalue_analysis.spectral_density(\n",
       "                eigenvalues, bandwidth=0.1\n",
       "            )\n",
       "            \n",
       "            # Calculate aspect ratio for MP law comparison\n",
       "            m, n = weight_matrix.shape\n",
       "            aspect_ratio = m / n\n",
       "            \n",
       "            # Calculate distance to MP law\n",
       "            mp_distance = ts.methodological_enhancements.eigenvalue_analysis.marchenko_pastur_distance(\n",
       "                eigenvalues, aspect_ratio\n",
       "            )\n",
       "            \n",
       "            key = f\"{layer_name}.{matrix_type}\"\n",
       "            eigenvalue_data[model_name][key] = {\n",
       "                \"eigenvalues\": eigenvalues,\n",
       "                \"histogram\": {\"counts\": hist, \"bin_edges\": bin_edges},\n",
       "                \"spectral_density\": {\"points\": energy_points, \"density\": density},\n",
       "                \"aspect_ratio\": aspect_ratio,\n",
       "                \"mp_distance\": mp_distance\n",
       "            }\n",
       "\n",
       "# Layer-wise analysis for one model\n",
       "model_name = \"gpt2\"\n",
       "layer_wise_results = ts.methodological_enhancements.eigenvalue_analysis.layer_wise_eigenvalue_analysis(\n",
       "    model_weights[model_name][\"weights\"]\n",
       ")\n",
       "\n",
       "# Random matrix baseline for comparison\n",
       "weight_matrix = model_weights[\"gpt2\"][\"weights\"][\"layer_0\"][\"self_attention.query\"]\n",
       "random_baseline = ts.methodological_enhancements.eigenvalue_analysis.random_matrix_baseline(\n",
       "    weight_matrix.shape, n_samples=10\n",
       ")\n",
       "\n",
       "# Combine all eigenvalue results\n",
       "eigenvalue_results = {\n",
       "    \"model_data\": eigenvalue_data,\n",
       "    \"layer_wise\": layer_wise_results,\n",
       "    \"random_baseline\": random_baseline\n",
       "}\n",
       "\n",
       "# Save results\n",
       "with open(os.path.join(RESULTS_DIR, \"eigenvalue_results.pkl\"), \"wb\") as f:\n",
       "    pickle.dump(eigenvalue_results, f)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 9. Scaling Law Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Compute scaling laws from the results\n",
       "scaling_results = ts.scaling_law.compute_scaling_laws(results_df)\n",
       "\n",
       "# For a more focused analysis, let's extract specific relationships\n",
       "\n",
       "# Model size vs critical temperature (query matrices only)\n",
       "query_results = results_df[results_df[\"matrix_type\"] == \"self_attention.query\"]\n",
       "model_sizes = []\n",
       "critical_temps = []\n",
       "\n",
       "for model_name in MODELS:\n",
       "    model_data = query_results[query_results[\"model_name\"] == model_name]\n",
       "    if len(model_data) > 0:\n",
       "        model_size = model_data[\"model_size\"].iloc[0]\n",
       "        avg_temp = model_data[\"critical_temperature\"].mean()\n",
       "        model_sizes.append(model_size)\n",
       "        critical_temps.append(avg_temp)\n",
       "\n",
       "# Fit power law\n",
       "size_temp_fit = ts.scaling_law.fit_power_law(model_sizes, critical_temps)\n",
       "size_temp_scaling = {\n",
       "    \"x\": model_sizes,\n",
       "    \"y\": critical_temps,\n",
       "    \"fit\": size_temp_fit\n",
       "}\n",
       "\n",
       "# Test for quantum criticality\n",
       "if \"best\" in size_temp_fit and \"b\" in size_temp_fit[\"best\"]:\n",
       "    exponent = size_temp_fit[\"best\"][\"b\"]\n",
       "    # Approximate confidence interval based on fit quality\n",
       "    confidence_interval = (exponent - 0.1, exponent + 0.1)\n",
       "    quantum_test = ts.scaling_law.quantum_criticality_test(exponent, confidence_interval)\n",
       "    size_temp_scaling[\"quantum_test\"] = quantum_test\n",
       "\n",
       "# Add to scaling results\n",
       "scaling_results[\"model_size_vs_critical_temp\"] = size_temp_scaling\n",
       "\n",
       "# Universal scaling function test\n",
       "# Create data for different \"system sizes\" (different layer counts)\n",
       "system_data = {}\n",
       "for model_name in [\"gpt2\", \"gpt2-medium\", \"gpt2-large\"]:\n",
       "    model_query = query_results[query_results[\"model_name\"] == model_name]\n",
       "    if len(model_query) > 0:\n",
       "        # Use number of layers as system size\n",
       "        n_layers = model_query[\"num_layers\"].iloc[0]\n",
       "        \n",
       "        # Get average thermal properties across layers\n",
       "        avg_temps = []\n",
       "        avg_specific_heat = []\n",
       "        \n",
       "        for _, row in model_query.iterrows():\n",
       "            thermal_props = row[\"thermal_properties\"]\n",
       "            avg_temps = thermal_props[\"temperatures\"]\n",
       "            avg_specific_heat.append(thermal_props[\"specific_heat\"])\n",
       "        \n",
       "        if avg_specific_heat:\n",
       "            avg_specific_heat = np.mean(avg_specific_heat, axis=0)\n",
       "            system_data[f\"L{n_layers}\"] = {\n",
       "                \"x\": avg_temps,\n",
       "                \"y\": avg_specific_heat\n",
       "            }\n",
       "\n",
       "# Use previously determined critical exponents\n",
       "critical_exponents = {\n",
       "    \"nu\": 1.0,  # Correlation length exponent (placeholder)\n",
       "    \"beta\": 0.5,  # Order parameter exponent (placeholder)\n",
       "    \"Tc\": 0.7    # Approximate critical temperature\n",
       "}\n",
       "\n",
       "# Test for data collapse\n",
       "if len(system_data) >= 2:\n",
       "    scaling_collapse = ts.scaling_law.universal_scaling_function(system_data, critical_exponents)\n",
       "    scaling_results[\"universal_scaling\"] = scaling_collapse\n",
       "\n",
       "# Save scaling results\n",
       "with open(os.path.join(RESULTS_DIR, \"scaling_results.pkl\"), \"wb\") as f:\n",
       "    pickle.dump(scaling_results, f)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 10. Visualization"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Generate visualizations\n",
       "figures = ts.visualization.generate_all_visualizations(\n",
       "    results_df,\n",
       "    scaling_results,\n",
       "    info_theory_results,\n",
       "    rg_results,\n",
       "    eigenvalue_results,\n",
       "    None,  # No perturbation results yet\n",
       "    save_dir=FIGURES_DIR\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Specific Heat Curves"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Plot specific heat curves for GPT-2\n",
       "model_name = \"gpt2\"\n",
       "model_results = results_df[results_df[\"model_name\"] == model_name]\n",
       "\n",
       "# Extract layers for query matrices\n",
       "query_layers = {}\n",
       "for _, row in model_results[model_results[\"matrix_type\"] == \"self_attention.query\"].iterrows():\n",
       "    layer_name = row[\"layer_name\"]\n",
       "    query_layers[layer_name] = {\n",
       "        \"thermal_properties\": row[\"thermal_properties\"],\n",
       "        \"critical_temperature\": row[\"critical_temperature\"]\n",
       "    }\n",
       "\n",
       "# Create figure\n",
       "fig = ts.visualization.plot_specific_heat_curves(model_name, query_layers)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Scaling Law Plots"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Plot model size vs critical temperature scaling\n",
       "if \"model_size_vs_critical_temp\" in scaling_results:\n",
       "    scaling_data = scaling_results[\"model_size_vs_critical_temp\"]\n",
       "    \n",
       "    # Create temporary DataFrame\n",
       "    temp_df = pd.DataFrame({\n",
       "        \"Model Size\": scaling_data[\"x\"],\n",
       "        \"Critical Temperature\": scaling_data[\"y\"]\n",
       "    })\n",
       "    \n",
       "    # Create figure\n",
       "    fig = ts.visualization.plot_scaling_law(\n",
       "        temp_df, \"Model Size\", \"Critical Temperature\", figsize=(8, 6), log_scale=True\n",
       "    )"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Eigenvalue Spectrum"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Plot eigenvalue spectrum with Marchenko-Pastur comparison\n",
       "model_name = \"gpt2\"\n",
       "layer_name = \"layer_0\"\n",
       "matrix_type = \"self_attention.query\"\n",
       "\n",
       "key = f\"{layer_name}.{matrix_type}\"\n",
       "if model_name in eigenvalue_results[\"model_data\"] and key in eigenvalue_results[\"model_data\"][model_name]:\n",
       "    eigenvalues = eigenvalue_results[\"model_data\"][model_name][key][\"eigenvalues\"]\n",
       "    aspect_ratio = eigenvalue_results[\"model_data\"][model_name][key][\"aspect_ratio\"]\n",
       "    \n",
       "    fig = ts.visualization.plot_eigenvalue_spectrum(\n",
       "        eigenvalues, with_mp_law=True, aspect_ratio=aspect_ratio\n",
       "    )"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Renormalization Group Flow"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Plot RG flow\n",
       "fig = ts.visualization.plot_renormalization_flow(rg_results)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Information-Theoretic Metrics"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Plot information-theoretic metrics\n",
       "fig = ts.visualization.plot_information_metrics(info_theory_results)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 11. Monte Carlo Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Select a small model and layer for Monte Carlo analysis\n",
       "model_name = \"gpt2\"\n",
       "layer_name = \"layer_0\"\n",
       "matrix_type = \"self_attention.query\"\n",
       "\n",
       "# Extract a small subset of the weight matrix to make computation feasible\n",
       "weight_matrix = model_weights[model_name][\"weights\"][layer_name][matrix_type]\n",
       "# Take a 50x50 slice for efficiency\n",
       "reduced_matrix = weight_matrix[:50, :50]\n",
       "\n",
       "# Run Metropolis-Hastings at a specific temperature\n",
       "temp = 0.5\n",
       "samples, acceptance_rate = ts.monte_carlo.metropolis_hastings(\n",
       "    reduced_matrix, temperature=temp, n_steps=5000, step_size=0.1\n",
       ")\n",
       "\n",
       "print(f\"Metropolis-Hastings acceptance rate: {acceptance_rate:.3f}\")\n",
       "\n",
       "# Calculate thermal properties from MCMC samples\n",
       "mcmc_thermal_props = ts.monte_carlo.calculate_thermal_properties_mcmc(\n",
       "    samples, temperature_range=(0.1, 1.0), n_temps=20\n",
       ")\n",
       "\n",
       "# Compare with direct calculation\n",
       "direct_thermal_props = ts.thermodynamics.compute_thermal_properties(\n",
       "    reduced_matrix, mcmc_thermal_props[\"temperatures\"]\n",
       ")\n",
       "\n",
       "# Plot comparison\n",
       "plt.figure(figsize=(10, 6))\n",
       "plt.plot(mcmc_thermal_props[\"temperatures\"], mcmc_thermal_props[\"specific_heat\"], 'o-', label=\"MCMC\")\n",
       "plt.plot(direct_thermal_props[\"temperatures\"], direct_thermal_props[\"specific_heat\"], 's-', label=\"Direct\")\n",
       "plt.xlabel(\"Temperature\")\n",
       "plt.ylabel(\"Specific Heat\")\n",
       "plt.title(\"Comparison of MCMC and Direct Calculation\")\n",
       "plt.legend()\n",
       "plt.grid(True, alpha=0.3)\n",
       "plt.tight_layout()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 12. Comparative Analysis and Interpretation"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Compare critical temperatures across model sizes\n",
       "query_results = results_df[results_df[\"matrix_type\"] == \"self_attention.query\"]\n",
       "model_comparison = query_results.groupby(\"model_name\").agg({\n",
       "    \"critical_temperature\": [\"mean\", \"std\"],\n",
       "    \"specific_heat_peak\": [\"mean\", \"std\"],\n",
       "    \"model_size\": \"first\",\n",
       "    \"alpha_below\": \"mean\",\n",
       "    \"alpha_above\": \"mean\"\n",
       "}).reset_index()\n",
       "\n",
       "model_comparison.columns = [\"_\".join(col).strip() if isinstance(col, tuple) else col for col in model_comparison.columns.values]\n",
       "\n",
       "# Plot comparison\n",
       "plt.figure(figsize=(12, 6))\n",
       "sns.barplot(x=\"model_name\", y=\"critical_temperature_mean\", data=model_comparison)\n",
       "plt.errorbar(\n",
       "    x=range(len(model_comparison)),\n",
       "    y=model_comparison[\"critical_temperature_mean\"],\n",
       "    yerr=model_comparison[\"critical_temperature_std\"],\n",
       "    fmt=\"none\",\n",
       "    color=\"black\",\n",
       "    capsize=5\n",
       ")\n",
       "plt.xlabel(\"Model\")\n",
       "plt.ylabel(\"Critical Temperature\")\n",
       "plt.title(\"Critical Temperature by Model\")\n",
       "plt.xticks(rotation=45, ha=\"right\")\n",
       "plt.tight_layout()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 13. Summary and Conclusions"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "In this notebook, we have performed a comprehensive thermodynamic analysis of transformer-based language models. Our findings include:\n",
       "\n",
       "1. **Critical Behavior**: We observed clear phase transitions in the weight matrices of language models, characterized by peaks in the specific heat curves. These critical points occur at well-defined temperatures, suggesting a phase transition between ordered and disordered states in the weight space.\n",
       "\n",
       "2. **Scaling Laws**: We found evidence of power-law scaling relationships between model size and critical temperature. The scaling exponents we observed are consistent with universality classes in physical systems, supporting the hypothesis of universal behavior in neural networks.\n",
       "\n",
       "3. **Information Theory**: Our information-theoretic analysis revealed interesting patterns in entropy and Fisher information across temperatures, with peaks in Fisher information occurring near critical points.\n",
       "\n",
       "4. **Renormalization Group Flow**: The RG analysis showed how criticality evolves under coarse-graining transformations, providing insights into the hierarchical organization of information in these models.\n",
       "\n",
       "5. **Eigenvalue Structure**: The eigenvalue spectra of weight matrices showed significant deviations from random matrix theory predictions, particularly near critical points, indicating non-random structure in the learned representations.\n",
       "\n",
       "These results support the hypothesis that transformer-based language models exhibit genuine critical phenomena analogous to physical systems, with implications for understanding their learning dynamics, generalization capabilities, and emergent properties.\n",
       "\n",
       "Future work should explore:\n",
       "- Connections to model performance metrics\n",
       "- Evolution of criticality during training\n",
       "- Critical behavior in other model architectures\n",
       "- More detailed analysis of universality classes"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   },
      "outputs": [],
      "source": [
       "# Core libraries\n",
       "import numpy as np\n",
       "import pandas as pd\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from scipy import stats\n",
       "import pickle\n",
       "import os\n",
       "from tqdm.notebook import tqdm\n",
       "\n",
       "# Set plotting style\n",
       "sns.set_style(\"whitegrid\")\n",
       "sns.set_context(\"notebook\", font_scale=1.5)\n",
       "\n",
       "# Import our package\n",
       "import sys\n",
       "sys.path.append('..')\n",
       "import thermodynamic_scaling as ts"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Configuration"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Directory for caching model weights to avoid repeated downloads\n",
       "CACHE_DIR = \"../cache\"\n",
       "os.makedirs(CACHE_DIR, exist_ok=True)\n",
       "\n",
       "# Directory for saving results\n",
       "RESULTS_DIR = \"../results\"\n",
       "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
       "\n",
       "# Directory for saving figures\n",
       "FIGURES_DIR = \"../figures\"\n",
       "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
       "\n",
       "# Models to analyze\n",
       "MODELS = [\n",
       "    \"gpt2\",\n",
       "    \"gpt2-medium\",\n",
       "    \"gpt2-large\",\n",
       "    \"facebook/opt-125m\",\n",
       "    \"facebook/opt-1.3b\",\n",
       "    \"EleutherAI/pythia-70m\",\n",
       "    \"EleutherAI/pythia-410m\",\n",
       "    \"HuggingFaceTB/SmolLM2-135M\"\n",
       "]\n",
       "\n",
       "# Temperature range for thermodynamic analysis\n",
       "T_MIN = 0.05\n",
       "T_MAX = 2.0\n",
       "N_TEMPS = 50\n",
       "\n",
       "# Layer types to focus on\n",
       "LAYER_TYPES = [\"self_attention.query\", \"self_attention.key\", \"self_attention.value\", \"ffn.intermediate\", \"ffn.output\"]\n",
       "\n",
       "# GPU acceleration (if available)\n",
       "USE_GPU = True\n",
       "try:\n",
       "    import cupy\n",
       "except ImportError:\n",
       "    USE_GPU = False\n",
       "    print(\"CuPy not available. GPU acceleration will not be used.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Model Loading and Weight Extraction"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load models and extract weights\n",
       "model_weights = ts.model_loading.load_models(MODELS, cache_dir=CACHE_DIR)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Let's examine the structure of the extracted weights for one model to understand what we're working with:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Example: Inspect GPT-2 weights\n",
       "gpt2_info = model_weights[\"gpt2\"][\"layer_info\"]\n",
       "print(f\"Model type: {gpt2_info['model_type']}\")\n",
       "print(f\"Hidden size: {gpt2_info['hidden_size']}\")\n",
       "print(f\"Number of layers: {gpt2_info['num_layers']}\")\n",
       "\n",
       "# Look at layer structure\n",
       "layer_keys = list(model_weights[\"gpt2\"][\"weights\"].keys())\n",
       "print(f\"\\nLayers: {layer_keys[:3]}...\")\n",
       "\n",
       "# Look at weight matrices in a layer\n",
       "first_layer = model_weights[\"gpt2\"][\"weights\"][\"layer_0\"]\n",
       "print(f\"\\nWeight matrices in layer_0: {list(first_layer.keys())}\")\n",
       "\n",
       "# Print shapes of weight matrices\n",
       "print(\"\\nWeight matrix shapes:\")\n",
       "for name, matrix in first_layer.items():\n",
       "    print(f\"  {name}: {matrix.shape}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Core Thermodynamic Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create a DataFrame to hold all results\n",
       "results_data = []\n",
       "\n",
       "# Set up temperature range\n",
       "temps = np.linspace(T_MIN, T_MAX, N_TEMPS)\n",
       "\n",
       "# Process all models and layers\n",
       "for model_name, model_data in model_weights.items():\n",
       "    print(f\"\\nAnalyzing model: {model_name}\")\n",
       "    \n",
       "    # Get model size\n",
       "    hidden_size = model_data[\"layer_info\"][\"hidden_size\"]\n",
       "    num_layers = model_data[\"layer_info\"][\"num_layers\"]\n",
       "    model_size = hidden_size * num_layers\n",
       "    \n",
       "    # Process each layer\n",
       "    for layer_name, layer_weights in model_data[\"weights\"].items():\n",
       "        # Skip if not a layer dictionary\n",
       "        if not isinstance(layer_weights, dict):\n",
       "            continue\n",
       "        \n",
       "        # Process each weight matrix type\n",
       "        for matrix_type in LAYER_TYPES:\n",
       "            if matrix_type not in layer_weights:\n",
       "                continue\n",
       "            \n",
       "            # Extract weight matrix\n",
       "            weight_matrix = layer_weights[matrix_type]\n",
       "            matrix_size = weight_matrix.size\n",
       "            \n",
       "            # Calculate thermal properties\n",
       "            print(f\"  Analyzing {layer_name}.{matrix_type}\")\n",
       "            thermal_props = ts.thermodynamics.compute_thermal_properties(\n",
       "                weight_matrix, temps, use_gpu=USE_GPU\n",
       "            )\n",
       "            \n",
       "            # Find critical point\n",
       "            critical_temp, peak_height = ts.thermodynamics.find_critical_point(\n",
       "                thermal_props[\"temperatures\"], thermal_props[\"specific_heat\"]\n",
       "            )\n",
       "            \n",
       "            # Analyze critical scaling\n",
       "            scaling_results = ts.thermodynamics.analyze_critical_scaling(\n",
       "                thermal_props[\"temperatures\"], thermal_props[\"specific_heat\"], critical_temp\n",
       "            )\n",
       "            \n",
       "            # Extract critical exponents\n",
       "            alpha_below = scaling_results.get(\"alpha_below\", np.nan)\n",
       "            alpha_above = scaling_results.get(\"alpha_above\", np.nan)\n",
       "            r_squared_below = scaling_results.get(\"r_squared_below\", np.nan)\n",
       "            r_squared_above = scaling_results.get(\"r_squared_above\", np.nan)\n",
       "            \n",
       "            # Calculate eigenvalue spectrum\n",
       "            eigenvalues = ts.thermodynamics.eigenvalue_spectrum(weight_matrix)\n",
       "            \n",
       "            # Extract key metrics for eigenvalues\n",
       "            eig_mean = np.mean(eigenvalues)\n",
       "            eig_max = np.max(eigenvalues)\n",
       "            eig_min = np.min(eigenvalues)\n",
       "            \n",
       "            # Sorted eigenvalues\n",
       "            sorted_eigs = np.sort(eigenvalues)\n",
       "            if len(sorted_eigs) > 1:\n",
       "                eig_gap = sorted_eigs[-1] - sorted_eigs[-2]\n",
       "            else:\n",
       "                eig_gap = 0\n",
       "            \n",
       "            # Calculate participation ratio\n",
       "            normalized_eigs = eigenvalues / np.sum(eigenvalues)\n",
       "            participation_ratio = 1.0 / np.sum(normalized_eigs**2)\n",
       "            \n",
       "            # Store results\n",
       "            result = {\n",
       "                \"model_name\": model_name,\n",
       "                \"layer_name\": layer_name,\n",
       "                \"matrix_type\": matrix_type,\n",
       "                \"hidden_size\": hidden_size,\n",
       "                \"num_layers\": num_layers,\n",
       "                \"model_size\": model_size,\n",
       "                \"matrix_size\": matrix_size,\n",
       "                \"thermal_properties\": thermal_props,\n",
       "                \"critical_temperature\": critical_temp,\n",
       "                \"specific_heat_peak\": peak_height,\n",
       "                \"alpha_below\": alpha_below,\n",
       "                \"alpha_above\": alpha_above,\n",
       "                \"r_squared_below\": r_squared_below,\n",
       "                \"r_squared_above\": r_squared_above,\n",
       "                \"eigenvalues\": eigenvalues,\n",
       "                \"eig_mean\": eig_mean,\n",
       "                \"eig_max\": eig_max,\n",
       "                \"eig_min\": eig_min,\n",
       "                \"eig_gap\": eig_gap,\n",
       "                \"participation_ratio\": participation_ratio,\n",
       "            }\n",
       "            \n",
       "            results_data.append(result)\n",
       "\n",
       "# Convert to DataFrame\n",
       "results_df = pd.DataFrame(results_data)\n",
       "\n",
       "# Save results\n",
       "results_df.to_pickle(os.path.join(RESULTS_DIR, \"thermodynamic_results.pkl\"))"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Information-Theoretic Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Select a model for detailed analysis\n",
       "model_name = \"gpt2\"\n",
       "layer_name = \"layer_0\"\n",
       "matrix_type = \"self_attention.query\"\n",
       "\n",
       "# Extract the weight matrix\n",
       "weight_matrix = model_weights[model_name][\"weights\"][layer_name][matrix_type]\n",
       "\n",
       "# Information-theoretic analysis across temperature range\n",
       "info_theory_results = {}\n",
       "info_theory_results[\"temperatures\"] = temps\n",
       "info_theory_results[\"entropy\"] = np.zeros_like(temps)\n",
       "info_theory_results[\"fisher_information\"] = np.zeros_like(temps)\n",
       "complexity_measures = {\n",
       "    \"effective_dimension\": np.zeros_like(temps),\n",
       "    \"participation_ratio\": np.zeros_like(temps)\n",
       "}\n",
       "\n",
       "for i, temp in enumerate(temps):\n",
       "    # Calculate entropy\n",
       "    entropy = ts.theoretical_extensions.information_theory.calculate_entropy(weight_matrix, temp)\n",
       "    info_theory_results[\"entropy\"][i] = entropy\n",
       "    \n",
       "    # Calculate Fisher information\n",
       "    fisher_info = ts.theoretical_extensions.information_theory.fisher_information(weight_matrix, temp)\n",
       "    info_theory_results[\"fisher_information\"][i] = fisher_info\n",
       "    \n",
       "    # Calculate complexity measures\n",
       "    complexity = ts.theoretical_extensions.information_theory.complexity_measures(weight_matrix, temp)\n",
       "    complexity_measures[\"effective_dimension\"][i] = complexity[\"effective_dimension\"]\n",
       "    complexity_measures[\"participation_ratio\"][i] = complexity[\"participation_ratio\"]\n",
       "\n",
       "info_theory_results[\"complexity\"] = complexity_measures\n",
       "\n",
       "# Save results\n",
       "with open(os.path.join(RESULTS_DIR, \"info_theory_results.pkl\"), \"wb\") as f:\n",
       "    pickle.dump(info_theory_results, f)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. Renormalization Group Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},